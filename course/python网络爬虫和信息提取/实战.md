# 实战

## 正则表达式

### 概念

简洁表达一组字符串的方式

通用的字符串表达框架

`p = re.compile('表达式')`

字符+操作符

```python
[abc]
[^abc]
abc*
abc+
abc?
(abc)?
^abc?$
```

### re库的使用

| 函数          | 说明                                | 函数       | 说明                                     |
| ------------- | ----------------------------------- | ---------- | ---------------------------------------- |
| re.search()   | 匹配正则表达式的第一个位置          | re.match() | 第一个字符开始匹配正则                   |
| re.findall()  | 列表返回全部匹配字符串              | re.split() | 将字符串按照正则结果分割                 |
| re.finditer() | 返回迭代类型，每个迭代都是match对象 | re.sub()   | 替换正则表达式的子串，返回替换后的字符串 |



```python
raw string类型
r'[1-9]\d{5}'

re.search(pattren, string, flags=0)
'''
	re.I/re.IGNORECASE ——忽略正则表达式的大小写
	re.M/re.MULTILINE ——正则表达式的^能够将给定字符串的每行开始匹配
	re.S/re.DOTALL ——正则表达式中的.能够匹配所有字符 包括换行符，默认匹配除换行外的所有字符
'''
import re
match = re.search(r'[1-9]\d{5}', 'BIT 100081')
if match:
    type(match)
    print(match.group(0)) # 100081
    
re.match(pattern,string,flags=0)
# maxsplit 剩下的所有作为一个部分
re.split(pattern,string,maxsplit=0,flags=0)

re.finditer(pattern,string,flags=0)
for m in re.finditer(...):
    if m:
        print m.group(0)
        
# 正则 替换匹配字符串的字符串 待匹配的字符串 控制标记
re.sub(pattern,repl,string,count=0,flags=0)
```

### 面向对象

`regex = re.compile(pattern,flags=0)`

> 将正则表达式的字符串形式编译成正则表达式对象

`regex = re.compile(r'[1-9]\d{5}')`

### match对象

| 属性    | 说明                         |      |
| ------- | ---------------------------- | ---- |
| .string | 待匹配的文本                 |      |
| .re     | 匹配时使用的pattern对象      |      |
| .pos    | 正则表达式搜索文本的开始位置 |      |
| .endpos | 正则表达式搜索文本的结束位置 |      |

| 方法      | 说明                             |
| --------- | -------------------------------- |
| .group(0) | 获取匹配后的字符串               |
| .start()  | 匹配字符串在原始字符串的开始位置 |
| .end()    | 匹配字符串在原始字符串的结束位置 |
| .span()   | 返回(.start(),.end())的元组类型  |

```python
import re
m = re.search(r'[1-9]\d{5}','BIT100081 TSU100084')
m.string
# > BIT100081 TSU100084
m.re
# re.compile('[1-9]\\d{5}')
m.pos # 0
m.endpos # 19 最后位置
m.group(0) # 返回第一次匹配结果
m.start() #3
m.end() #9
m.span() #(3,9)
```

### 贪婪匹配和最小匹配

> re库默认采用贪婪模式，最长匹配

- 最小匹配操作符

  | 操作符 | 说明                                 |
  | ------ | ------------------------------------ |
  | *?     | 前一个字符0次或无限次扩展，最小匹配  |
  | +?     | 前一个字符1次或无限次扩展，最小匹配  |
  | ??     | 前一个字符0次或1次扩展，最小匹配     |
  | {m,n}? | 扩展前一个字符m至n次（含n)，最小匹配 |

  

## 实例一

```python
import requests
import re


def get_html_text(url):
    try:
        r = requests.get(url,timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""
    
def parse_page(ilt,html):
    try:
        # 价格 xx.xx
        plt = re.findall(r'\"view_price\"\:\"[\d.]*\"',html) 
        # 名称
        tlt = re.findall(r'\"raw_title\"\:\".*?\"',html) 
        for i in range(len(plt)):
            # eval 去除'
            price = eval(plt[i].split(':')[1])
            title = eval(tlt[i].split(':')[1])
            ilt.append([price,title])
    except:
    	print("")
        
        
def print_goods_list(ilt):
    tplt = "{:4}\t{:8}\t:{6}"
    print(tplt.format("序号","价格","商品名称"))
    count = 0
    for good in ilt :
        count++
        print(tplt.format(count,good[0],good[1]))
    
def main():
    goods = "书包"
    depth = 2
    start_url = "https://s.taobao.com/search/?q="+goods
    info_list = []
    for i in range(depth):
        try:
            url = start_url+'&s='+str(44*i)
            html = get_html_text(url)
    		parse_page(info_list,html)
        except:
            continue
    print_goods_list(info_list)
    
if __name__ == '__main__':
    main()
```



## 实例二

```python
import requests
from bs4 import BeautifulSoup
import traceback
import re


def get_html_text(url):
    try:
        r = requests.get(url,timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""
    
def get_stock_list(lst,stock_url):
    html = get_html_text(stock_url)
    soup = BeautifulSoup(html,"html.parser")
    a_list = soup.find_all('a')
    for a in a_list:
        try:
            href = a.attr['href']
            lst.append(re.findall(r'[s][hz]\d{6}',href)[0])
        except:
            continue

def get_stock_info(lst,stock_url,file_path):
    for stock in lst:
        url = stock_url + stock + '.html'
        html = get_html_text(url)
        try:
            if html == "":
                continue
            else:
                infoDict = {}
                soup = BeautifulSoup(html,"html.parser")
                # stock_info = soup.find('div',attrs={class:"stock-bets"})
                stock_info = soup.find('div',class_="stock-bets")
                
                name = stock_info.find_all(attrs={'class':'bets-name'})[0]
                info_dict.update({'股票名称':name.text.split()[0]})
        except:
            
    return ""

def main():
    stock_list_url = "http://www.aaa.com/aaaxxx"
    stock_url="aa"
    file_path="D://aa"
    s_list=[]
    get_stock_list(s_list, stock_list_url)
    get_stock_info(s_list,stock_url,file_path)
    
if __name__ == '__main__':
    main()
```

