# 规则

## Requests库

- **安装**

  ```bash
  pip install requests
  ```

- **测试**

  ```python
  import requests
  r = requests.get('http://www.baidu.com')
  println(r.status_code)
  type(r)
  ```

  | 方法                 | 说明         |
  | -------------------- | ------------ |
  | `requests.request()` | 创建一个请求 |
  | `requests.get()`     | ..           |
  | `requests.head()`    | ..           |
  | `requests.post()`    | ..           |
  | `requests.put()`     | ..           |
  | `requests.patch()`   | ..           |
  | `requests.delete()`  | ..           |

- **get方法**

  ```python
  r = requests.get(url,params=None,**kwargs) 
  '''
  url:链接
  params:可选额外参数，字典或字节流格式
  **kwargs： 12个控制访问的参数
  response对象
  同 requests.request('get',url,params=None,**kwargs) 
  '''
  r.headers # 
  ```

  | 属性                  | 说明                         |
  | --------------------- | ---------------------------- |
  | `r.status_code`       | 返回状态码                   |
  | `r.text`              | 响应内容的字符串格式         |
  | `r.encoding`          | header中的响应内容编码方式   |
  | `r.apparent_encoding` | 内容分析出的响应内容编码方式 |
  | `r.content`           | 响应内容二进制格式           |

- **异常处理**

  | 异常                        | 说明                              |
  | --------------------------- | --------------------------------- |
  | `requests.ConnectionError`  | 网络连接错误，DNS、服务器拒绝连接 |
  | `requests.HTTPError`        | HTTP异常                          |
  | `requests.URLRequired`      | URL缺失异常                       |
  | `requests.TooManyRedirects` | 用户重定向过多异常                |
  | `requests.ConnectTimeout`   | 远程服务器连接超时异常            |
  | `requests.Timeout`          | 发起请求到接收响应超时异常        |

  `r.raise_for_status` 如果不是200，产生异常`requests.HTTPError`

- **通用代码框架**

  ```python
  import requests
  
  def getHTMLText(url):
      try:
          r = requests.get(url,timeout=30)
          r.raise_for_status() # 如果不是200.抛出HTTPError异常
          r.encoding = r.apparent_encoding
          return r.text
      except:
          return "异常"
      
  if __name__ == "__main__":
      url = "http://www.baidu.com"
      println(getHTMLText(url))
      
  ```

- **http**

  略

- **post/put**

  ```python
  import requests
  
  payload = {'key1':'value1','key2':'value2'}
  r = requests.post('http://aaa.com/post',data = payload)
  r.text
  ```

- **requests.request**

  ```python
  requests.request(method,url,**kwargs)
  '''
  **kwargs:
      params: url 后面的键值对
      data: body里面的参数 表单格式
      json: body json格式
      headers: 请求定制头信息 用于模拟浏览器
      cookies 
      auth: 认证
      files: 上传文件
      fs={'file':open('data.xls','rb')}...files=fs
      timeout 设置超时时间
      proxies 字典,增加代理
      pxs = {'http':'http://user:pass@10.10.10.10:1234',
      'https':'https://10.10.10.1:4321'} 隐藏用户ip地址
      allow_redirects 重定向 boolean
      stream 获取内容立即下载 boolean
      verify ssl证书开关 boolean
      cert 保存本地ssl证书路径
  '''
  r = requests.request("get","http://www.baidu.com",params={"k":"v"},data={},json={})
  ```

  

## 实例

| 分类/规模 | 库       | 规模     |
| --------- | -------- | -------- |
| 小规模    | requests | 爬取网页 |
| 中等规模  | scrapy   | 爬取网站 |
| 大规模    | 定制开发 | 爬取全网 |

- 限制

  来源审查—— User-Agent

  发布公告——Robots协议

  ​	网站告知可爬虫网页，网站根目录的robots.txt目录可访问

